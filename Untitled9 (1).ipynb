{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0atp9X60JAjN"
      },
      "outputs": [],
      "source": [
        "#filter method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "The Filter method is a commonly used feature selection technique in machine learning, where features are selected based on their statistical characteristics with respect to the target variable, independently of the model being used. It ranks features by evaluating their relevance, removing irrelevant or redundant features before training the model. This method is particularly useful for reducing overfitting, improving model accuracy, and decreasing computational time.\n",
        "\n",
        "How Does the Filter Method Work?\n",
        "The Filter method works by applying statistical tests to measure the relationship between each feature and the target variable. Based on these statistics, the features are ranked, and only the most relevant ones are retained for model training. The process is independent of the learning algorithm, making it faster and simpler compared to model-based feature selection methods like wrapper or embedded techniques.\n",
        "\n",
        "Key Statistical Metrics Used in the Filter Method:\n",
        "Correlation Coefficient (for numeric data):\n",
        "\n",
        "Pearson correlation coefficient for continuous features to assess the linear relationship between each feature and the target variable.\n",
        "Spearman’s rank correlation for non-linear relationships.\n",
        "Chi-Square Test (for categorical data):\n",
        "\n",
        "Measures the dependence between the categorical features and the target variable. It checks whether a feature is independent of the target or not.\n",
        "Mutual Information:\n",
        "\n",
        "Measures the amount of information obtained about the target variable through the feature. It works for both continuous and categorical data.\n",
        "Variance Threshold:\n",
        "\n",
        "Features with low variance are considered uninformative and can be removed (e.g., features where most of the data points have the same value).\n",
        "ANOVA F-Value:\n",
        "\n",
        "Measures the variance between different groups in a feature (for continuous data) and how well they explain the target variable"
      ],
      "metadata": {
        "id": "UvDLixL7JXDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#difference between wrapper method and filter method"
      ],
      "metadata": {
        "id": "88DVKAjHJiNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Both the Wrapper method and Filter method are feature selection techniques in machine learning, but they differ significantly in their approach, execution, and computational complexity.\n",
        "\n",
        "1. Filter Method:\n",
        "The Filter method evaluates features based on their statistical relationship with the target variable before building any machine learning model. It is independent of the learning algorithm and focuses on the intrinsic characteristics of the data.\n",
        "\n",
        "How it works: Features are ranked based on a specific statistical criterion, such as correlation, chi-square, mutual information, etc.\n",
        "Example criteria: Pearson correlation (for continuous features), chi-square test (for categorical features), ANOVA, variance threshold, etc.\n",
        "Advantages:\n",
        "Fast and simple: It’s computationally less expensive because it does not require training the model.\n",
        "Model-agnostic: The method is independent of any learning algorithm, so it can be applied to any dataset.\n",
        "Reduces overfitting risk: Since it doesn’t use the model in the selection process, it has less risk of overfitting.\n",
        "Disadvantages:\n",
        "Ignores interactions between features: Features are evaluated independently, so interactions or combinations of features might not be captured.\n",
        "Not as tailored to a specific model: Since it does not consider the performance of the model, the selected features might not always be the best for a specific algorithm.\n",
        "2. Wrapper Method:\n",
        "The Wrapper method evaluates features by iteratively training the machine learning model on different subsets of features and selecting the subset that gives the best model performance. It is model-dependent and focuses on optimizing the model's performance for a given set of features.\n",
        "\n",
        "How it works: The method “wraps” the feature selection process around the learning algorithm. It tries various combinations of features, trains the model with each combination, and selects the subset that yields the best model performance (e.g., highest accuracy, lowest error, etc.).\n",
        "\n",
        "Common Wrapper techniques include:\n",
        "\n",
        "Forward Selection: Start with an empty feature set, add one feature at a time, and keep the one that improves the model performance.\n",
        "Backward Elimination: Start with all features and remove one feature at a time that decreases model performance the least.\n",
        "Recursive Feature Elimination (RFE): Recursively remove the least important features and retrain the model."
      ],
      "metadata": {
        "id": "Hh90gtuSJyuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embeedded feature selection methods"
      ],
      "metadata": {
        "id": "tnFc7QXaJ_hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Embedded feature selection methods integrate the process of feature selection directly into the model training. These methods select features during the process of model training, rather than before (as in the Filter method) or after (as in the Wrapper method). This approach is computationally efficient because it combines feature selection with model training and can help prevent overfitting.\n",
        "\n",
        "Common Techniques Used in Embedded Feature Selection\n",
        "Regularization Techniques (Lasso and Ridge Regression): Regularization adds a penalty term to the loss function of a machine learning model to constrain the magnitude of the model parameters. This encourages the model to prefer simpler features, effectively performing feature selection.\n",
        "\n",
        "Lasso Regression (L1 regularization): Lasso (Least Absolute Shrinkage and Selection Operator) adds an L1 penalty to the model, which can shrink some coefficients to exactly zero. Features with zero coefficients are effectively eliminated. This is a powerful method for selecting important features in linear models.\n",
        "\n",
        "Lasso Loss Function: Minimize\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eRmx4uFBKT35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drawbacks of filter method"
      ],
      "metadata": {
        "id": "hxOBeTSmK2cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The Filter method for feature selection, while fast and simple, comes with several drawbacks that may affect the performance and accuracy of a machine learning model. Here are the primary drawbacks of using the Filter method for feature selection:\n",
        "\n",
        "1. Ignores Feature Interactions\n",
        "The Filter method evaluates each feature individually, without considering the potential interactions between features. In real-world problems, multiple features often interact with each other, and their combined effect could be more important than each feature's individual contribution."
      ],
      "metadata": {
        "id": "RvYQmtLqLINo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}