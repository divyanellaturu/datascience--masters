{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#handling missing values"
      ],
      "metadata": {
        "id": "Btg_Mv4FplnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Missing values in a dataset occur when certain data points are absent or unavailable in one or more columns. They are represented as NaN (Not a Number), None, or blanks, depending on the dataset format.\n",
        "\n",
        "For example:\n",
        "\n",
        "ID\tAge\tGender\tSalary\n",
        "1\t35\tMale\t50,000\n",
        "2\tNaN\tFemale\t70,000\n",
        "3\t28\tNaN\tNaN\n",
        "4\t45\tMale\t90,000\n",
        "In this case, values are missing for Age, Gender, and Salary."
      ],
      "metadata": {
        "id": "kya2xFddqTtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Handling missing values is crucial because:\n",
        "\n",
        "Model Performance: Many machine learning algorithms do not handle missing values natively and will either throw an error or provide inaccurate predictions if the missing data is not addressed.\n",
        "\n",
        "Data Integrity: Missing data can introduce bias and affect the generalizability of the model if not treated correctly. For example, dropping too many rows due to missing values could shrink the dataset significantly.\n",
        "\n",
        "Feature Engineering: Many data transformations and feature engineering steps (like scaling, encoding, etc.) require complete data.\n",
        "\n",
        "Statistical Analysis: In statistical models or even basic summary statistics, missing values can lead to incorrect measures of central tendency (mean, median, mode) and dispersion (variance, standard deviation).\n",
        "\n",
        "Algorithms that are not affected by missing values:\n",
        "While many algorithms require data to be complete, some models can handle missing data inherently:\n",
        "\n",
        "Decision Trees (CART, Random Forests, Gradient Boosting Trees): These tree-based models can work with missing values by ignoring them or using strategies like surrogates for splitting the nodes.\n",
        "\n",
        "Random Forest: Some implementations of Random Forest can handle missing values internally by considering surrogate splits.\n",
        "k-Nearest Neighbors (KNN): KNN can handle missing values by using distance metrics that skip missing features or by imputing them on the fly using neighboring data points.\n",
        "\n",
        "Naive Bayes: Some implementations of Naive Bayes can ignore missing values while computing probabilities for classification tasks.\n",
        "\n",
        "XGBoost: XGBoost has an inbuilt method to handle missing values by treating them as a separate value (default direction). The algorithm automatically learns how to treat missing values during the tree-building process."
      ],
      "metadata": {
        "id": "hX2e-reeqppX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing values handling techniques"
      ],
      "metadata": {
        "id": "cknh4hLwq0qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Handling missing data is an essential part of data preprocessing in machine learning and statistics. Here are common techniques to handle missing data, along with Python code examples:\n",
        "\n",
        "1. Removing Missing Data\n",
        "Remove rows with missing values.\n",
        "Remove columns with a significant proportion of missing values."
      ],
      "metadata": {
        "id": "SdiFKr4lq_mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "data = {'Name': ['John', 'Anna', 'Peter', np.nan],\n",
        "        'Age': [28, np.nan, 34, 45],\n",
        "        'Salary': [50000, 60000, np.nan, 70000]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Remove rows with any missing values\n",
        "df_drop_rows = df.dropna()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fHriCXBerdI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#handling imbalanced data"
      ],
      "metadata": {
        "id": "h-k2SkR1sRUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Imbalanced data refers to a dataset where the distribution of classes is unequal. In classification problems, it occurs when one class (called the minority class) has significantly fewer samples than the other class (called the majority class). This is common in scenarios like fraud detection, disease diagnosis, or spam classification.\n",
        "\n",
        "For example, in a binary classification problem:\n",
        "\n",
        "Class 1 (Minority): 100 samples (positive cases, like fraud).\n",
        "Class 0 (Majority): 10,000 samples (negative cases, like non-fraud).\n",
        "This imbalance can range from slight to extreme, and the algorithms might struggle to correctly classify the minority class.\n",
        "\n",
        "Why is Imbalanced Data a Problem?\n",
        "Machine learning algorithms, especially those focused on classification, tend to maximize accuracy by predicting the majority class more often. This can lead to misleading results where the model performs well on the majority class but poorly on the minority class.\n",
        "\n",
        "Consequences of Not Handling Imbalanced Data:\n",
        "High Accuracy, Poor Precision: If not handled, the model might predict most samples as the majority class and achieve high overall accuracy, but it would fail to identify minority class instances effectively.\n",
        "\n",
        "For example, if 95% of your data belongs to Class 0, a model can achieve 95% accuracy by always predicting Class 0, but it would fail to recognize Class 1, the minority class, which could be the more critical class (e.g., fraudulent transactions).\n",
        "Poor Recall for the Minority Class: The model might miss many minority class samples, resulting in poor recall (i.e., the ability to find all the actual positive cases). This is crucial in applications like medical diagnoses or fraud detection, where false negatives are highly undesirable.\n",
        "\n",
        "Bias Towards Majority Class: Most models, especially those like logistic regression and decision trees, might become biased towards the majority class. They would predict the majority class more often, skewing the decision boundaries, thus not capturing important patterns for the minority class.\n",
        "\n",
        "Misleading Evaluation Metrics: Standard metrics like accuracy can be misleading. For imbalanced datasets, accuracy is not a good measure because it focuses on the correct classification of both classes, regardless of distribution. You could have high accuracy but a very poor F1 score, precision, or recall for the minority class."
      ],
      "metadata": {
        "id": "vKv4pmTQuyPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upsampling & down sampling"
      ],
      "metadata": {
        "id": "rXxdB-bNvKlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Up-sampling and down-sampling are two common techniques used to handle imbalanced datasets, specifically by modifying the dataset's class distribution.\n",
        "\n",
        "Up-sampling (Over-sampling):\n",
        "\n",
        "This involves increasing the number of instances of the minority class by duplicating existing data points or generating synthetic data points.\n",
        "Goal: To balance the dataset by making the minority class as frequent as the majority class.\n",
        "Down-sampling (Under-sampling):\n",
        "\n",
        "This involves reducing the number of instances of the majority class by randomly removing samples.\n",
        "Goal: To balance the dataset by reducing the size of the majority class.\n",
        "When are Up-sampling and Down-sampling Required?\n",
        "These techniques are used when you're dealing with imbalanced datasets where one class significantly outnumbers the other(s), which can lead to poor performance of machine learning models, especially for the minority class. The goal is to make the model treat both classes with equal importance."
      ],
      "metadata": {
        "id": "r82PNJoFvPY1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}